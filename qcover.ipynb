{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "94e339c9-0701-4b8a-9f3b-1973a4f6c360",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dqn test for cover trading\n",
    "# referenceï¼š https://pytorch-lightning.readthedocs.io/en/latest/notebooks/lightning_examples/reinforce-learning-DQN.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "4d81147f-eee5-47c0-a8ed-de4ae6c61cb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from collections import OrderedDict, deque, namedtuple\n",
    "from typing import List, Tuple\n",
    "import os, sys\n",
    "sys.path.append('/mnt/data/projects/wankun01/workdir/playground/dqn/venv/lib/python3.9/site-packages')\n",
    "import gym\n",
    "from gym import spaces\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from pytorch_lightning import LightningModule, Trainer\n",
    "from pytorch_lightning.utilities import DistributedType\n",
    "from torch import Tensor, nn\n",
    "from torch.optim import Adam, Optimizer\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data.dataset import IterableDataset\n",
    "\n",
    "PATH_DATASETS = os.environ.get(\"PATH_DATASETS\", \".\")\n",
    "AVAIL_GPUS = min(1, torch.cuda.device_count())\n",
    "\n",
    "# !pip install git+https://github.com/PytorchLightning/lightning-bolts.git@master --upgrade"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c898533a-78a8-4652-94be-4ab599a7b0e0",
   "metadata": {},
   "source": [
    "# model setup and preparation of lightning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "107f2101-6f9c-4009-981d-6f6befc3aeb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQN(nn.Module):\n",
    "    \"\"\"Simple MLP network.\"\"\"\n",
    "\n",
    "    def __init__(self, obs_size: int, n_actions: int, hidden_size: int = 128):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            obs_size: observation/state size of the environment\n",
    "            n_actions: number of discrete actions available in the environment\n",
    "            hidden_size: size of hidden layers\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(obs_size, hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_size, n_actions),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x.float())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c4a5617b-3d39-4d68-b88a-4c7a0d0fbf45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Named tuple for storing experience steps gathered in training\n",
    "Experience = namedtuple(\n",
    "    \"Experience\",\n",
    "    field_names=[\"state\", \"action\", \"reward\", \"done\", \"new_state\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dc9d0865-8999-4500-af06-cbf1e051c59c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayBuffer:\n",
    "    \"\"\"Replay Buffer for storing past experiences allowing the agent to learn from them.\n",
    "\n",
    "    Args:\n",
    "        capacity: size of the buffer\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, capacity: int) -> None:\n",
    "        self.buffer = deque(maxlen=capacity)\n",
    "\n",
    "    def __len__(self) -> None:\n",
    "        return len(self.buffer)\n",
    "\n",
    "    def append(self, experience: Experience) -> None:\n",
    "        \"\"\"Add experience to the buffer.\n",
    "\n",
    "        Args:\n",
    "            experience: tuple (state, action, reward, done, new_state)\n",
    "        \"\"\"\n",
    "        self.buffer.append(experience)\n",
    "\n",
    "    def sample(self, batch_size: int) -> Tuple:\n",
    "        indices = np.random.choice(len(self.buffer), batch_size, replace=False)\n",
    "        states, actions, rewards, dones, next_states = zip(*(self.buffer[idx] for idx in indices))\n",
    "\n",
    "        return (\n",
    "            np.array(states),\n",
    "            np.array(actions),\n",
    "            np.array(rewards, dtype=np.float32),\n",
    "            np.array(dones, dtype=np.bool),\n",
    "            np.array(next_states),\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5b26f4fd-ca5f-42b5-8507-a546e6be2734",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RLDataset(IterableDataset):\n",
    "    \"\"\"Iterable Dataset containing the ExperienceBuffer which will be updated with new experiences during training.\n",
    "\n",
    "    Args:\n",
    "        buffer: replay buffer\n",
    "        sample_size: number of experiences to sample at a time\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, buffer: ReplayBuffer, sample_size: int = 200) -> None:\n",
    "        self.buffer = buffer\n",
    "        self.sample_size = sample_size\n",
    "\n",
    "    def __iter__(self):\n",
    "        states, actions, rewards, dones, new_states = self.buffer.sample(self.sample_size)\n",
    "        for i in range(len(dones)):\n",
    "            yield states[i], actions[i], rewards[i], dones[i], new_states[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7efc82fb-2b42-4c49-ba05-e7712244e417",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent:\n",
    "    \"\"\"Base Agent class handeling the interaction with the environment.\"\"\"\n",
    "\n",
    "    def __init__(self, env: gym.Env, replay_buffer: ReplayBuffer) -> None:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            env: training environment\n",
    "            replay_buffer: replay buffer storing experiences\n",
    "        \"\"\"\n",
    "        self.env = env\n",
    "        self.replay_buffer = replay_buffer\n",
    "        self.reset()\n",
    "        self.state = self.env.reset()\n",
    "\n",
    "    def reset(self) -> None:\n",
    "        \"\"\"Resents the environment and updates the state.\"\"\"\n",
    "        self.state = self.env.reset()\n",
    "\n",
    "    def get_action(self, net: nn.Module, epsilon: float, device: str) -> int:\n",
    "        \"\"\"Using the given network, decide what action to carry out using an epsilon-greedy policy.\n",
    "\n",
    "        Args:\n",
    "            net: DQN network\n",
    "            epsilon: value to determine likelihood of taking a random action\n",
    "            device: current device\n",
    "\n",
    "        Returns:\n",
    "            action\n",
    "        \"\"\"\n",
    "        if np.random.random() < epsilon:\n",
    "            # randomly take an action\n",
    "            action = self.env.action_space.sample()\n",
    "        else:\n",
    "            # use model defined action\n",
    "            state = torch.tensor([self.state])\n",
    "            if device not in [\"cpu\"]:\n",
    "                state = state.cuda(device)\n",
    "\n",
    "            q_values = net(state)\n",
    "            _, action = torch.max(q_values, dim=1) # take the larges q-value action\n",
    "            action = int(action.item())\n",
    "\n",
    "        return action\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def play_step(\n",
    "        self,\n",
    "        net: nn.Module,\n",
    "        epsilon: float = 0.0,\n",
    "        device: str = \"cpu\",\n",
    "    ) -> Tuple[float, bool]:\n",
    "        \"\"\"Carries out a single interaction step between the agent and the environment.\n",
    "\n",
    "        Args:\n",
    "            net: DQN network\n",
    "            epsilon: value to determine likelihood of taking a random action\n",
    "            device: current device\n",
    "\n",
    "        Returns:\n",
    "            reward, done\n",
    "        \"\"\"\n",
    "\n",
    "        action = self.get_action(net, epsilon, device)\n",
    "\n",
    "        # do step in the environment (it's env's job and return the new state and reward)\n",
    "        new_state, reward, done, _ = self.env.step(action)\n",
    "\n",
    "        exp = Experience(self.state, action, reward, done, new_state)\n",
    "\n",
    "        self.replay_buffer.append(exp)\n",
    "\n",
    "        # replace the state with th e new state\n",
    "        self.state = new_state\n",
    "        if done:\n",
    "            self.reset()\n",
    "        return reward, done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "14160132-8096-40fc-a81e-55a5b72efb54",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQNLightning(LightningModule):\n",
    "    \"\"\" Basic DQN model\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self, \n",
    "        batch_size:int = 16,\n",
    "        lr:float = 1e-2,\n",
    "        env = None,\n",
    "        gamma: float = 0.99,\n",
    "        sync_rate:int = 10,\n",
    "        replay_size:int = 1000,\n",
    "        warm_start_size:int = 1000,\n",
    "        eps_last_frame: int = 1000,\n",
    "        eps_start: float = 1.0,\n",
    "        eps_end: float = 0.01,\n",
    "        episode_length:int = 200,\n",
    "        warm_start_steps:int = 1000,\n",
    "    ) -> None:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            batch_size: size of the batches\")\n",
    "            lr: learning rate\n",
    "            env: gym environment tag\n",
    "            gamma: discount factor\n",
    "            sync_rate: how many frames do we update the target network\n",
    "            replay_size: capacity of the replay buffer\n",
    "            warm_start_size: how many samples do we use to fill our buffer at the start of training\n",
    "            eps_last_frame: what frame should epsilon stop decaying\n",
    "            eps_start: starting value of epsilon\n",
    "            eps_end: final value of epsilon\n",
    "            episode_length: max length of an episode\n",
    "            warm_start_steps: max episode reward in the environment\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters()\n",
    "        \n",
    "        # self.env = gym.make(self.hparams.env) # if use env tag\n",
    "        self.env = env # if directly parsing an env\n",
    "        obs_size = self.env.observation_space.shape[0]\n",
    "        n_actions = self.env.action_space.shape[0]\n",
    "        print(f'obs_size: {obs_size}')\n",
    "        print(f'n_actions: {n_actions}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a65a4edf-5397-45ea-8f57-81491736d6a2",
   "metadata": {},
   "source": [
    "# customized env setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "83a30038-4d70-41d8-9f6c-417ade1e03c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_ACCOUNT_BALANCE = 2147483647\n",
    "MAX_NUM_SHARES = 2147483647\n",
    "MAX_SHARE_PRICE = 5000\n",
    "MAX_OPEN_POSITIONS = 5\n",
    "MAX_STEPS = 20000\n",
    "\n",
    "INITIAL_ACCOUNT_BALANCE = 10000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "fee9e980-681b-4f0c-85e4-6c283b44b200",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FXTradingEnv(gym.Env):\n",
    "    \"A prop trading env for FX \"\n",
    "    metadata = {'render.modes': ['human']}\n",
    "    \n",
    "    def __init__(self, df):\n",
    "        super().__init__()\n",
    "        self.df = df \n",
    "        self.reward_range = (0, MAX_ACCOUNT_BALANCE)\n",
    "        \n",
    "        \n",
    "        # todo: get to understand the meaning of space.Box() classs\n",
    "        # actions\n",
    "        self.action_space = spaces.Box(low=np.array([0,0]), high=np.array([3,1]), dtype=np.float16)\n",
    "        \n",
    "        # prices of OHCL values for the last five entries\n",
    "        self.observation_space = spaces.Box(low=0, high=1, shape=(6, 6), dtype=np.float16)\n",
    "        \n",
    "    def reset(self):\n",
    "        # reset to init state\n",
    "        self.balance = INITIAL_ACCOUNT_BALANCE\n",
    "        self.net_worth = INITIAL_ACCOUNT_BALANCE\n",
    "        self.max_net_worth = INITIAL_ACCOUNT_BALANCE\n",
    "        self.shares_held = 0\n",
    "        self.cost_basis = 0\n",
    "        self.total_shares_sold = 0\n",
    "        self.total_sales_value = 0\n",
    "        \n",
    "        # set the current step to a random point within the data frame to start the new round.\n",
    "        self.current_step = random.randint(0, len(self.df.loc[:, 'Open'].values)-6)\n",
    "        return self._next_observation()\n",
    "    \n",
    "    def _next_observation(self):\n",
    "        # get last five steps and make the next obs\n",
    "        # try minmax scaler later\n",
    "        # why not + 6\n",
    "        frame = np.array([\n",
    "            self.df.loc[self.current_step: self.current_step + 5, 'Open'].values / MAX_SHARE_PRICE,\n",
    "            self.df.loc[self.current_step: self.current_step + 5, 'High'].values / MAX_SHARE_PRICE,\n",
    "            self.df.loc[self.current_step: self.current_step + 5, 'Low'].values / MAX_SHARE_PRICE,\n",
    "            self.df.loc[self.current_step: self.current_step + 5, 'Close'].values / MAX_SHARE_PRICE,\n",
    "            self.df.loc[self.current_step: self.current_step + 5, 'Volume'].values / MAX_SHARE_PRICE,\n",
    "        ])\n",
    "        obs = np.append(frame, \n",
    "                        [[\n",
    "                        self.balance / MAX_ACCOUNT_BALANCE,\n",
    "                        self.max_net_worth / MAX_ACCOUNT_BALANCE,\n",
    "                        self.shares_held / MAX_NUM_SHARES,\n",
    "                        self.cost_basis / MAX_SHARE_PRICE,\n",
    "                        self.total_shares_sold / MAX_NUM_SHARES,\n",
    "                        self.total_sales_value / (MAX_NUM_SHARES * MAX_SHARE_PRICE),\n",
    "                        ]],\n",
    "                        axis = 0\n",
    "                       )\n",
    "        return obs\n",
    "    \n",
    "    def step(self, action):\n",
    "        # every step gaps 5 frames\n",
    "        self._take_action(action)\n",
    "        self.current_step += 1\n",
    "        \n",
    "        if self.current_step > len(self.df.loc[:, 'Open'].values)-6:\n",
    "            self.current_step = 0\n",
    "            \n",
    "        delay_modifier = (self.current_step / MAX_STEPS)\n",
    "        # balance will be changed due to self._take_action, so as net_worth\n",
    "        reward = self.balance * delay_modifier \n",
    "        done = self.net_worth <= 0\n",
    "        \n",
    "        obs = self._next_observation() # after updating the step\n",
    "        # obs is new_state when call step \n",
    "        return obs, reward, done, {}\n",
    "    \n",
    "    \n",
    "    \n",
    "    def _take_action(self, action):\n",
    "        # either buy, sell, hold\n",
    "        # why not high or low\n",
    "        # needs to seperate ask and bid prices\n",
    "        current_price = random.uniform(\n",
    "            self.df.loc[current_step, 'Open'],\n",
    "            self.df.loc[current_step, 'Close']\n",
    "        )\n",
    "        \n",
    "        action_type = action[0]\n",
    "        amount = action[1]\n",
    "        \n",
    "        if action_type < 1:\n",
    "            # buy amount % of balance in shares\n",
    "            total_possible = self.balance / current_price\n",
    "            share_bought = total_possible * amount\n",
    "            prev_cost = self.cost_basis * self.shares_held\n",
    "            additional_cost = shares_bought * current_price\n",
    "            self.balance -= addtional_cost\n",
    "            self.cost_basis = (prev_cost + additional_cost) / (self.shares_held + share_bought)\n",
    "            self.share_held += share_bought\n",
    "        elif action_type < 2:\n",
    "            # sell\n",
    "            shares_sold = self.shares_hold * amount\n",
    "            self.balance += shares_sold * current_price\n",
    "            self.shares_held -= shares_sold\n",
    "            self.total_shares_sold += shares_sold\n",
    "            self.total_sales_value += shares_sold * current_price\n",
    "            \n",
    "        self.net_worth = self.balance + self.shares_held * current_price\n",
    "        if self.net_worth > self.max_net_worth:\n",
    "            self.max_net_worth = self.net_worth\n",
    "        \n",
    "        if self.shares_held == 0:\n",
    "            self.cost_basis = 0\n",
    "    \n",
    "    \n",
    "    def render(self, mode='human', close=False):\n",
    "        # render the environment to the screen \n",
    "        profit = self.net_worth - INITIAL_ACCOUNT_BALANCE\n",
    "        \n",
    "        print(f'Step: {self.current_step}')\n",
    "        print(f'Balance: {self.balance}')\n",
    "        print(f'Shares held: {self.shares_held} (Total sold: {self.total-shares_sold})')\n",
    "        print(f'Avg cost for held shares: {self.cost_basis} (Total sales value: {self.total_sales_value})')\n",
    "        print(f'Net worth: {self.net_worth} (Max net worth: {self.max_net_worth})')\n",
    "        print(f'Profit: {profit}')\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0daa1a98-9f9e-4492-920a-9dca05e17778",
   "metadata": {},
   "source": [
    "# build up algo "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "19383845-a93a-4707-9675-05a2eb391159",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>Date</th>\n",
       "      <th>Open</th>\n",
       "      <th>High</th>\n",
       "      <th>Low</th>\n",
       "      <th>Close</th>\n",
       "      <th>Volume</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1998-01-02</td>\n",
       "      <td>13.63</td>\n",
       "      <td>16.25</td>\n",
       "      <td>13.50</td>\n",
       "      <td>16.25</td>\n",
       "      <td>6411700.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1998-01-05</td>\n",
       "      <td>16.50</td>\n",
       "      <td>16.56</td>\n",
       "      <td>15.19</td>\n",
       "      <td>15.88</td>\n",
       "      <td>5820300.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>1998-01-06</td>\n",
       "      <td>15.94</td>\n",
       "      <td>20.00</td>\n",
       "      <td>14.75</td>\n",
       "      <td>18.94</td>\n",
       "      <td>16182800.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>1998-01-07</td>\n",
       "      <td>18.81</td>\n",
       "      <td>19.00</td>\n",
       "      <td>17.31</td>\n",
       "      <td>17.50</td>\n",
       "      <td>9300200.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>1998-01-08</td>\n",
       "      <td>17.44</td>\n",
       "      <td>18.62</td>\n",
       "      <td>16.94</td>\n",
       "      <td>18.19</td>\n",
       "      <td>6910900.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0        Date   Open   High    Low  Close      Volume\n",
       "0           0  1998-01-02  13.63  16.25  13.50  16.25   6411700.0\n",
       "1           1  1998-01-05  16.50  16.56  15.19  15.88   5820300.0\n",
       "2           2  1998-01-06  15.94  20.00  14.75  18.94  16182800.0\n",
       "3           3  1998-01-07  18.81  19.00  17.31  17.50   9300200.0\n",
       "4           4  1998-01-08  17.44  18.62  16.94  18.19   6910900.0"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('AAPL.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "4bbd7886-66d6-4763-8a45-da9faa4dbb0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "obs_size: 6\n",
      "n_actions: 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/data/projects/wankun01/workdir/playground/dqn/venv/lib/python3.9/site-packages/gym/spaces/box.py:84: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float16\u001b[0m\n",
      "  logger.warn(f\"Box bound precision lowered by casting to {self.dtype}\")\n"
     ]
    }
   ],
   "source": [
    "# todo: understand the meanning of the action and obs spaces\n",
    "fx_trading_env = FXTradingEnv(df)\n",
    "lightning_module = DQNLightning(env=fx_trading_env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "b54086c7-3826-415f-841d-bd953b085e26",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Box(0.0, [3. 1.], (2,), float16)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fx_trading_env.action_space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "432e7aa8-8862-4913-bd1a-07581355da48",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rl",
   "language": "python",
   "name": "rl"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
